{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12429090,"sourceType":"datasetVersion","datasetId":7839774}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here are several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n       os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:06.326185Z","iopub.execute_input":"2025-08-15T11:46:06.326784Z","iopub.status.idle":"2025-08-15T11:46:11.578875Z","shell.execute_reply.started":"2025-08-15T11:46:06.326759Z","shell.execute_reply":"2025-08-15T11:46:11.578219Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# Image Classification\nimport tensorflow as tf\n\ndata_dir = \"/kaggle/input/fresh-rotten-and-formalin-mixed-fruit-detection/Fruits Original-ML-GRP-07\"\n\ndataset = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    image_size=(128, 128),  # resize all images\n    batch_size=32           # number of images per batch\n)\n\nclass_names = dataset.class_names\nprint(\"Classes:\", class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:11.580012Z","iopub.execute_input":"2025-08-15T11:46:11.580214Z","iopub.status.idle":"2025-08-15T11:46:12.051604Z","shell.execute_reply.started":"2025-08-15T11:46:11.580199Z","shell.execute_reply":"2025-08-15T11:46:12.050882Z"}},"outputs":[{"name":"stdout","text":"Found 10153 files belonging to 5 classes.\nClasses: ['Apple', 'Banana', 'Grape', 'Mango', 'Orange']\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"# ============================================================\n# BYOL (from scratch, PyTorch) + Linear Evaluation on Fruits\n# ============================================================\n\n# 0) (Colab) Ensure deps (torch/torchvision are preinstalled on Colab)\n# !pip install torch torchvision --quiet\n\nimport os, copy, math, random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.052568Z","iopub.execute_input":"2025-08-15T11:46:12.052816Z","iopub.status.idle":"2025-08-15T11:46:12.056833Z","shell.execute_reply.started":"2025-08-15T11:46:12.052794Z","shell.execute_reply":"2025-08-15T11:46:12.056154Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"# 1) Reproducibility + Device\nseed = 123\nrandom.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.058345Z","iopub.execute_input":"2025-08-15T11:46:12.058950Z","iopub.status.idle":"2025-08-15T11:46:12.074742Z","shell.execute_reply.started":"2025-08-15T11:46:12.058934Z","shell.execute_reply":"2025-08-15T11:46:12.074199Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"# 2) Paths & Hyperparams\ndata_dir = \"/kaggle/input/fresh-rotten-and-formalin-mixed-fruit-detection/Fruits Original-ML-GRP-07\"\nimage_size = 128\nbatch_size_pretrain = 64     # reduce if you see CUDA OOM\nbatch_size_finetune = 32\nepochs_pretrain = 10         # increase for better representations\nepochs_finetune = 15\nm_ema = 0.996                # BYOL EMA momentum\nproj_hidden = 1024           # MLP hidden\nproj_out = 256               # projection / prediction dim\nbase_lr = 1e-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.075309Z","iopub.execute_input":"2025-08-15T11:46:12.075492Z","iopub.status.idle":"2025-08-15T11:46:12.101917Z","shell.execute_reply.started":"2025-08-15T11:46:12.075478Z","shell.execute_reply":"2025-08-15T11:46:12.101318Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"# 3) BYOL Augmentations (TwoCropsTransform)\nimagenet_mean = (0.485, 0.456, 0.406)\nimagenet_std  = (0.229, 0.224, 0.225)\n\naugmentation = transforms.Compose([\n    transforms.RandomResizedCrop(image_size, scale=(0.2, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n])\n\nclass TwoCropsTransform:\n    \"\"\"Create two differently augmented views of the same image.\"\"\"\n    def __init__(self, base_transform):\n        self.base_transform = base_transform\n    def __call__(self, x):\n        return self.base_transform(x), self.base_transform(x)\n\n# For fine-tuning (no heavy augs)\neval_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.102501Z","iopub.execute_input":"2025-08-15T11:46:12.102851Z","iopub.status.idle":"2025-08-15T11:46:12.118822Z","shell.execute_reply.started":"2025-08-15T11:46:12.102826Z","shell.execute_reply":"2025-08-15T11:46:12.118264Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"# 4) Datasets & Loaders\n# Pretraining dataset returns ((x1, x2), label) â€” label is ignored\npretrain_dataset = datasets.ImageFolder(root=data_dir, transform=TwoCropsTransform(augmentation))\npretrain_loader = DataLoader(\n    pretrain_dataset,\n    batch_size=batch_size_pretrain,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n    drop_last=True,  # important for BN stability\n)\n\n# For linear eval (labeled)\nfull_supervised = datasets.ImageFolder(root=data_dir, transform=eval_transform)\nnum_classes = len(full_supervised.classes)\nprint(\"Detected classes:\", full_supervised.classes)\n\n# Split 80/20 for train/test\ntrain_len = int(0.8 * len(full_supervised))\ntest_len = len(full_supervised) - train_len\nsup_train, sup_test = random_split(full_supervised, [train_len, test_len])\ntrain_loader = DataLoader(sup_train, batch_size=batch_size_finetune, shuffle=True, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(sup_test,  batch_size=batch_size_finetune, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.119555Z","iopub.execute_input":"2025-08-15T11:46:12.119762Z","iopub.status.idle":"2025-08-15T11:46:12.313459Z","shell.execute_reply.started":"2025-08-15T11:46:12.119740Z","shell.execute_reply":"2025-08-15T11:46:12.312760Z"}},"outputs":[{"name":"stdout","text":"Detected classes: ['Apple', 'Banana', 'Grape', 'Mango', 'Orange']\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"# 5) Backbone (ResNet18 without classifier)\ndef get_backbone():\n    resnet = models.resnet18(weights=None)  # weights=\"IMAGENET1K_V1\" is okay too if you want a warm-start\n    backbone = nn.Sequential(*list(resnet.children())[:-1])  # to get (B, 512, 1, 1)\n    feat_dim = 512\n    return backbone, feat_dim\n\nbackbone_online, feat_dim = get_backbone()\nbackbone_target = copy.deepcopy(backbone_online)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.314361Z","iopub.execute_input":"2025-08-15T11:46:12.314830Z","iopub.status.idle":"2025-08-15T11:46:12.505994Z","shell.execute_reply.started":"2025-08-15T11:46:12.314804Z","shell.execute_reply":"2025-08-15T11:46:12.505219Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"# 6) BYOL MLP Heads\nclass MLP(nn.Module):\n    \"\"\"2-layer MLP with BatchNorm, used for projector and predictor.\"\"\"\n    def __init__(self, in_dim, hidden_dim, out_dim, last_bn=True):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(hidden_dim, out_dim)\n        self.last_bn = last_bn\n        if last_bn:\n            self.bn2 = nn.BatchNorm1d(out_dim, affine=False)  # projector's last BN (no affine) as in BYOL\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        if self.last_bn:\n            x = self.bn2(x)\n        return x\n\nprojector_online = MLP(feat_dim, proj_hidden, proj_out, last_bn=True)\npredictor_online = MLP(proj_out, proj_hidden, proj_out, last_bn=False)  # predictor has no last BN\nprojector_target = copy.deepcopy(projector_online)\n\n# Target network does not get gradients\nfor p in backbone_target.parameters():  p.requires_grad = False\nfor p in projector_target.parameters(): p.requires_grad = False\n\n# Move to device\nbackbone_online.to(device)\nprojector_online.to(device)\npredictor_online.to(device)\nbackbone_target.to(device)\nprojector_target.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.506946Z","iopub.execute_input":"2025-08-15T11:46:12.507232Z","iopub.status.idle":"2025-08-15T11:46:12.561914Z","shell.execute_reply.started":"2025-08-15T11:46:12.507204Z","shell.execute_reply":"2025-08-15T11:46:12.561199Z"}},"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"MLP(\n  (fc1): Linear(in_features=512, out_features=1024, bias=True)\n  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n)"},"metadata":{}}],"execution_count":117},{"cell_type":"code","source":"# 7) Helper functions (EMA update + BYOL loss)\n@torch.no_grad()\ndef update_moving_average(m: float):\n    \"\"\"EMA update for target net parameters from online net parameters.\"\"\"\n    for online, target in zip(backbone_online.parameters(), backbone_target.parameters()):\n        target.data = target.data * m + online.data * (1.0 - m)\n    for online, target in zip(projector_online.parameters(), projector_target.parameters()):\n        target.data = target.data * m + online.data * (1.0 - m)\n\ndef flatten_features(x):\n    # x: (B, C, 1, 1) -> (B, C)\n    return torch.flatten(x, 1)\n\ndef l2_normalize(x, dim=1, eps=1e-8):\n    return x / (x.norm(p=2, dim=dim, keepdim=True) + eps)\n\ndef byol_loss_fn(p, z):\n    \"\"\"Negative cosine similarity between p (online prediction) and z (target projection, stop-grad).\"\"\"\n    p = l2_normalize(p, dim=1)\n    z = l2_normalize(z, dim=1)\n    # cosine similarity -> mean over batch; BYOL uses 2 - 2 * cos, but minimizing -cos is equivalent up to a constant\n    return 2 - 2 * (p * z).sum(dim=1).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.563555Z","iopub.execute_input":"2025-08-15T11:46:12.563998Z","iopub.status.idle":"2025-08-15T11:46:12.569283Z","shell.execute_reply.started":"2025-08-15T11:46:12.563979Z","shell.execute_reply":"2025-08-15T11:46:12.568720Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"# 8) Optimizer\nparams = list(backbone_online.parameters()) + list(projector_online.parameters()) + list(predictor_online.parameters())\noptimizer = torch.optim.Adam(params, lr=base_lr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.570051Z","iopub.execute_input":"2025-08-15T11:46:12.570877Z","iopub.status.idle":"2025-08-15T11:46:12.590268Z","shell.execute_reply.started":"2025-08-15T11:46:12.570856Z","shell.execute_reply":"2025-08-15T11:46:12.589758Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"# 9) BYOL Pretraining\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# 1. Define the optimizer first\nparams = list(backbone_online.parameters()) + \\\n         list(projector_online.parameters()) + \\\n         list(predictor_online.parameters())\noptimizer = optim.AdamW(params, lr=3e-4, weight_decay=1e-6)\n\n# 2. Initialize the scheduler\nscheduler = CosineAnnealingLR(optimizer, T_max=epochs_pretrain)\n\n# 3. Training loop\nprint(\"=== BYOL pretraining ===\")\nfor epoch in range(1, epochs_pretrain + 1):\n    running = 0.0\n    m = momentum_by_epoch(epoch, epochs_pretrain)\n\n    for (x1, x2), _ in pretrain_loader:\n        x1 = x1.to(device, non_blocking=True)\n        x2 = x2.to(device, non_blocking=True)\n\n        # Online forward pass\n        y1 = flatten(backbone_online(x1))\n        z1 = projector_online(y1)\n        p1 = predictor_online(z1)\n        \n        y2 = flatten(backbone_online(x2))\n        z2 = projector_online(y2)\n        p2 = predictor_online(z2)\n\n        # Target forward pass (no gradients)\n        with torch.no_grad():\n            y1_t = flatten(backbone_target(x1))\n            z1_t = projector_target(y1_t)\n            y2_t = flatten(backbone_target(x2))\n            z2_t = projector_target(y2_t)\n\n        loss = byol_loss(p1, z2_t) + byol_loss(p2, z1_t)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(params, 5.0)\n        optimizer.step()\n\n        ema_update(m)\n        running += loss.item()\n\n    # Update learning rate\n    scheduler.step()\n    \n    print(f\"Epoch [{epoch}/{epochs_pretrain}]  BYOL Loss: {running/len(pretrain_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:46:12.591084Z","iopub.execute_input":"2025-08-15T11:46:12.591330Z","iopub.status.idle":"2025-08-15T12:14:59.525069Z","shell.execute_reply.started":"2025-08-15T11:46:12.591310Z","shell.execute_reply":"2025-08-15T12:14:59.524142Z"}},"outputs":[{"name":"stdout","text":"=== BYOL pretraining ===\nEpoch [1/10]  BYOL Loss: 2.6630\nEpoch [2/10]  BYOL Loss: 1.4661\nEpoch [3/10]  BYOL Loss: 1.1096\nEpoch [4/10]  BYOL Loss: 0.9146\nEpoch [5/10]  BYOL Loss: 0.8353\nEpoch [6/10]  BYOL Loss: 0.7257\nEpoch [7/10]  BYOL Loss: 0.6791\nEpoch [8/10]  BYOL Loss: 0.6121\nEpoch [9/10]  BYOL Loss: 0.5893\nEpoch [10/10]  BYOL Loss: 0.5692\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"# 10) Linear Evaluation / Fine-tuning\n# --- freeze encoder and fix it in eval mode ---\nfor p in backbone_online.parameters():\n    p.requires_grad = False\nbackbone_online.eval()  # VERY IMPORTANT: keep BN frozen\n\nlinear_head = nn.Linear(feat_dim, num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # mild smoothing helps\noptimizer_cls = torch.optim.SGD(\n    linear_head.parameters(),\n    lr=0.1, momentum=0.9, weight_decay=1e-4\n)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_cls, T_max=epochs_finetune * len(train_loader)\n)\n\nprint(\"\\n=== Linear evaluation (backbone frozen, BN fixed) ===\")\nfor epoch in range(1, epochs_finetune + 1):\n    linear_head.train()                  # ONLY the head trains\n    running_loss, correct, total = 0.0, 0, 0\n\n    for imgs, labels in train_loader:\n        imgs = imgs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n\n        with torch.no_grad():            # no grads & no BN updates in the encoder\n            feats = torch.flatten(backbone_online(imgs), 1)\n\n        logits = linear_head(feats)\n        loss = criterion(logits, labels)\n\n        optimizer_cls.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer_cls.step()\n        scheduler.step()\n\n        running_loss += loss.item()\n        preds = logits.argmax(1)\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n\n    acc = 100.0 * correct / total\n    print(f\"Epoch [{epoch}/{epochs_finetune}]  Loss: {running_loss/len(train_loader):.4f}  Train Acc: {acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:14:59.526335Z","iopub.execute_input":"2025-08-15T12:14:59.527096Z","iopub.status.idle":"2025-08-15T12:36:20.324739Z","shell.execute_reply.started":"2025-08-15T12:14:59.527067Z","shell.execute_reply":"2025-08-15T12:36:20.323966Z"}},"outputs":[{"name":"stdout","text":"\n=== Linear evaluation (backbone frozen, BN fixed) ===\nEpoch [1/15]  Loss: 3.8085  Train Acc: 72.29%\nEpoch [2/15]  Loss: 2.6392  Train Acc: 79.25%\nEpoch [3/15]  Loss: 1.9428  Train Acc: 82.46%\nEpoch [4/15]  Loss: 2.2512  Train Acc: 81.30%\nEpoch [5/15]  Loss: 2.0031  Train Acc: 82.49%\nEpoch [6/15]  Loss: 1.5654  Train Acc: 85.04%\nEpoch [7/15]  Loss: 1.1389  Train Acc: 87.02%\nEpoch [8/15]  Loss: 0.8832  Train Acc: 88.44%\nEpoch [9/15]  Loss: 0.7867  Train Acc: 89.01%\nEpoch [10/15]  Loss: 0.5492  Train Acc: 92.45%\nEpoch [11/15]  Loss: 0.4959  Train Acc: 93.60%\nEpoch [12/15]  Loss: 0.4561  Train Acc: 94.19%\nEpoch [13/15]  Loss: 0.4250  Train Acc: 95.00%\nEpoch [14/15]  Loss: 0.4129  Train Acc: 95.16%\nEpoch [15/15]  Loss: 0.4053  Train Acc: 95.27%\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"# 11) Evaluation on hold-out set\n# First define a simple classifier on top of BYOL features\nclass LinearClassifier(nn.Module):\n    def __init__(self, feat_dim=512, num_classes=10):\n        super().__init__()\n        self.fc = nn.Linear(feat_dim, num_classes)\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# Initialize classifier\nclf = LinearClassifier().to(device)\n\n# Train classifier (simplified example)\nbackbone_online.eval()  # Use the trained BYOL backbone\noptimizer = torch.optim.Adam(clf.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nprint(\"Training classifier...\")\nfor epoch in range(10):  # Short classifier training\n    for imgs, labels in train_loader:\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        \n        with torch.no_grad():\n            features = flatten(backbone_online(imgs))\n        \n        logits = clf(features)\n        loss = criterion(logits, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Now evaluate\nclf.eval()\ntotal, correct = 0, 0\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        \n        # Extract features using BYOL backbone\n        features = flatten(backbone_online(imgs))\n        \n        # Classify\n        logits = clf(features)\n        preds = logits.argmax(1)\n        \n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n\nprint(f\"\\nTest Accuracy: {100.0 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T13:05:02.660530Z","iopub.execute_input":"2025-08-15T13:05:02.661062Z","iopub.status.idle":"2025-08-15T13:19:33.068533Z","shell.execute_reply.started":"2025-08-15T13:05:02.661038Z","shell.execute_reply":"2025-08-15T13:19:33.067728Z"}},"outputs":[{"name":"stdout","text":"Training classifier...\n\nTest Accuracy: 89.76%\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}